# Modified DSSFN Architecture for Hyperspectral Image Classification

This project implements a modified version of the DSSFN (Dual-Stream Self-Attention Fusion Network) architecture, based on the paper "DSSFN: A Dual-Stream Self-Attention Fusion Network for Effective Hyperspectral Image Classification" (Yang et al., Remote Sensing 2023), for the classification of Hyperspectral Images (HSI).

## Overview

Hyperspectral images contain rich spectral information, enabling detailed analysis of materials on the Earth's surface. This project utilizes deep learning to classify HSI pixels. The DSSFN architecture processes both spectral and spatial information using two parallel streams.

**Key Components of the Original Architecture:**

* **Dual-Stream:** One stream processes 1D spectral vectors, the other processes 2D spatial patches.
* **Pyramidal Residual Blocks:** Used in both streams to extract features at different levels with residual connections.
* **Self-Attention:** Integrated within the residual blocks to identify the importance of different parts of the spectral/spatial input.

**Modifications in this Implementation:**

* **Optional Intermediate Cross-Attention:** Allows adding `MultiHeadCrossAttention` layers between streams after intermediate stages (Stage 1 and/or Stage 2) for early information exchange.
* **Configurable Fusion Mechanism:** Provides a choice between:
    * `AdaptiveWeight`: Weights the outputs (logits) of each stream based on their current loss.
    * `CrossAttention`: Uses `MultiHeadCrossAttention` to fuse the final features from both streams before a single classification layer.
* **Learned Positional Embeddings:** Used in conjunction with cross-attention (intermediate and final).
* **Flexible Band Selection:** Supports `SWGMF`, `E-FDPC` methods, or using all original bands.

A detailed technical description of the modified architecture can be found in the `dssfn_modified_description_uk` document (if available in your environment).

## Project Structure

term-paper/│├── data/                     # Directory for HSI datasets (e.g., Indian Pines, Pavia)│   ├── ip/                   # Example: Indian Pines dataset directory│   │   ├── indianpinearray.npy # HSI data cube│   │   └── IPgt.npy          # Ground truth labels│   ├── pu/                   # Example: Pavia University dataset directory│   │   └── ...│   └── botswana/             # Example: Botswana dataset directory│   │   └── ...│   └── ...                   # Other dataset directories│├── results/                  # Directory for outputs (logs, models, plots, results)│   ├── [DATASET_NAME]/       # Subdirectory per dataset│   │   ├── run_[TIMESTAMP]/  # Outputs for a specific single run│   │   │   ├── run_log.txt│   │   │   ├── run_config.json│   │   │   ├── [DATASET_NAME]_best_model.pth (optional)│   │   │   ├── [DATASET_NAME]test_results.txt│   │   │   ├── test_predictions.npy│   │   │   ├── test_labels.npy│   │   │   ├── [DATASET_NAME]training_history.png│   │   │   └── [DATASET_NAME]classification_map.png (if generated by main.py)│   │   ├── sweep[SWEEP_TYPE][TIMESTAMP]/ # Outputs for a sweep run│   │   │   └── ...│   │   └── classification_maps/ # Dedicated folder for maps from generate_classification_maps.py│   │       └── maprun[TIMESTAMP]/│   │           ├── [DATASET_NAME]_classification_map.png│   │           └── [DATASET_NAME]map_run_config.json│   └── classification_map_generation/ # Logs for the map generation script│       └── map_generation_log[TIMESTAMP].txt│├── scripts/                  # Executable scripts│   ├── main.py               # Main script for single training/evaluation run│   ├── sweep_bands.py        # Script for SWGMF band sweep experiment│   ├── sweep_bands_efdpc.py  # Script for E-FDPC dc_percent sweep experiment│   ├── sweep_combinations.py # Script for combination sweep (e.g., Indian Pines)│   ├── sweep_combinations_pu.py # Script for Pavia Uni combination sweep│   ├── sweep_combinations_botswana.py # Script for Botswana combination sweep│   ├── generate_classification_maps.py # Script to generate maps for all datasets│   └── inspect_botswana_dataset.py # Utility to inspect .mat dataset files│├── src/                      # Source code modules│   ├── init.py│   ├── config.py             # Configuration settings│   ├── data_utils.py         # Data loading, normalization, padding, patching│   ├── band_selection.py     # SWGMF and E-FDPC implementation│   ├── sampling.py           # Data splitting and coordinate handling│   ├── datasets.py           # PyTorch Dataset and DataLoader creation│   ├── model.py              # DSSFN model definition│   ├── modules.py            # Core building blocks (Attention, ResBlocks)│   ├── engine.py             # Training and evaluation loops│   └── visualization.py      # Plotting functions│├── .gitignore                # Git ignore file├── requirements.txt          # Project dependencies└── README.md                 # This file
## Requirements

* Python 3.x
* PyTorch
* NumPy
* Scikit-learn
* Matplotlib
* Pandas (for sweep result tables)

**Installation:**

1.  **Create a Virtual Environment (Recommended):**
    ```bash
    python -m venv .venv
    source .venv/bin/activate  # On Windows use `.venv\Scripts\activate`
    ```
2.  **Install Dependencies:**
    Install the required packages using the provided `requirements.txt` file:
    ```bash
    pip install -r requirements.txt
    ```
    *(Ensure you have the correct base Python and pip versions. The `requirements.txt` file specifies exact versions used during development. You might need to install the correct PyTorch version separately based on your CUDA setup if the one in `requirements.txt` is not compatible: see https://pytorch.org/)*

## Usage

### 1. Data Preparation

* Place your HSI datasets (in `.npy` or `.mat` format) inside the `data/` directory, organized into subdirectories (e.g., `data/ip/`, `data/pu/`, `data/botswana/`).
* Update the relevant dataset parameters (`DATASET_NAME`, `DATA_FILE`, `GT_FILE`, `NUM_CLASSES`, `EXPECTED_DATA_SHAPE`, `EXPECTED_GT_SHAPE`, `CLASS_NAMES`, `DATA_MAT_KEY`, `GT_MAT_KEY`) in `src/config.py` if you are running scripts like `main.py` that rely on a single dataset configuration. The `generate_classification_maps.py` script and sweep scripts for specific datasets (e.g., `sweep_combinations_pu.py`) manage dataset configurations internally.

### 2. Configuration (`src/config.py`)

Modify `src/config.py` to set up your experiment for scripts like `main.py` or as a base for sweep scripts:

* **Dataset:** Set `DATASET_NAME` and related paths/parameters.
* **Band Selection:**
    * `BAND_SELECTION_METHOD`: Choose `'SWGMF'`, `'E-FDPC'`, or `'None'`.
    * `SWGMF_TARGET_BANDS`: Set the desired number of bands if using SWGMF.
    * `E_FDPC_DC_PERCENT`: Set the percentage for E-FDPC cutoff distance calculation.
* **Patch Extraction:** Set `PATCH_SIZE`.
* **Data Splitting:** Configure `TRAIN_RATIO`, `VAL_RATIO`, `RANDOM_SEED`.
* **Model Architecture:**
    * `SPEC_CHANNELS`, `SPATIAL_CHANNELS`: Define channel dimensions per stage.
    * `INTERMEDIATE_ATTENTION_STAGES`: List of stages (e.g., `[1]`, `[1, 2]`, `[]`) after which intermediate cross-attention is applied.
    * `FUSION_MECHANISM`: Choose `'AdaptiveWeight'` or `'CrossAttention'` for the final fusion step.
    * `CROSS_ATTENTION_HEADS`, `CROSS_ATTENTION_DROPOUT`: Parameters for cross-attention modules.
* **Training Hyperparameters:** Set `EPOCHS`, `LEARNING_RATE`, `WEIGHT_DECAY`, `OPTIMIZER_TYPE`, `USE_SCHEDULER`, etc.
* **Early Stopping:** Configure `EARLY_STOPPING_ENABLED`, `EARLY_STOPPING_PATIENCE`, etc.
* **Output:** `SAVE_BEST_MODEL` controls saving the model with the best validation metric.

### 3. Running Experiments

Navigate to the `scripts/` directory in your terminal (ensure your virtual environment is activated).

* **Single Run (`main.py`):**
    * Configure `src/config.py` for the desired dataset and parameters.
    ```bash
    python main.py
    ```
    Results will be saved in `results/[DATASET_NAME]/run_[TIMESTAMP]/`.

* **Sweep Scripts (e.g., `sweep_bands.py`, `sweep_combinations.py`):**
    * These scripts often override parts of `src/config.py` (like band selection method or dataset) internally to iterate through parameter combinations.
    * Modify the sweep parameters directly within the respective sweep script (e.g., `band_sweep_values` in `sweep_bands.py`).
    ```bash
    python sweep_combinations_pu.py 
    # or python sweep_bands_efdpc.py, etc.
    ```
    Results and summary CSV/TXT files will be saved in `results/[DATASET_NAME]/sweep_[SWEEP_TYPE]_[TIMESTAMP]/`.

* **Generate Classification Maps (`generate_classification_maps.py`):**
    * This script iterates through predefined dataset configurations (Indian Pines, Pavia University, Botswana) and trains a model using a fixed configuration (E-FDPC band selection, intermediate attention after Stage 1, AdaptiveWeight fusion).
    * It then generates and saves classification maps.
    ```bash
    python generate_classification_maps.py
    ```
    Output maps and their run configurations are saved in `results/[DATASET_NAME]/classification_maps/maprun_[TIMESTAMP]/`. A general log for the script execution is saved in `results/classification_map_generation/`.

### 4. Checking Results

* Outputs for each run (logs, configuration details, evaluation metrics, plots, saved models) are stored in the `results/` directory.
* **Single Runs (`main.py`):** Check `results/[DATASET_NAME]/run_[TIMESTAMP]/` for detailed logs, config JSON, test results text file, and plots.
* **Sweep Runs:** Check `results/[DATASET_NAME]/sweep_[SWEEP_TYPE]_[TIMESTAMP]/` for aggregated results in `.csv` and `.txt` formats, along with detailed logs.
* **Classification Maps (`generate_classification_maps.py`):** Check `results/[DATASET_NAME]/classification_maps/maprun_[TIMESTAMP]/` for the saved `.png` classification maps and the `_map_run_config.json` detailing the settings for that specific map.

